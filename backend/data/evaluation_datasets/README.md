# Evaluation Datasets

This directory contains generated evaluation datasets for testing and validating the AI agents.

## Structure

```
evaluation_datasets/
├── needle_agent/          # Datasets generated by the needle agent
│   ├── client1_report1_tourAndCarePolicy.jsonl
│   ├── client2_report2_tourAndCarePolicy.jsonl
│   ├── client3_report3_MenoraPolicy.jsonl
│   └── ...
├── summary_agent/         # Datasets generated by the summary agent (future)
└── tableQA_agent/         # Datasets generated by the tableQA agent (future)
```

## File Naming Convention

Datasets are named after the source PDF files to maintain clear relationships:
- Format: `{pdf_filename}.jsonl`
- Example: `client1_report1_tourAndCarePolicy.jsonl`
- Each PDF gets its own JSONL file, even when processing multiple PDFs

## Usage

### Generating Datasets

```bash
# Generate dataset for a single PDF (creates client1_report1_tourAndCarePolicy.jsonl)
python agents/needle_agent/needle_dataset_synthesize_cli.py --pdf-path data/insurance/client1_report1_tourAndCarePolicy.pdf

# Generate datasets for all PDFs in a directory (creates separate JSONL for each PDF)
python agents/needle_agent/needle_dataset_synthesize_cli.py --pdf-directory data/insurance/

# Generate datasets with custom output directory
python agents/needle_agent/needle_dataset_synthesize_cli.py --pdf-directory data/insurance/ --output-path my_custom_directory/
```

### Dataset Format

Each line in the JSONL file contains a JSON object with:
- `question`: The generated question
- `answer`: The expected answer
- `context`: The source context/chunk used
- Additional metadata as needed

## Version Control

Consider adding this directory to `.gitignore` if the datasets become large or contain sensitive information. For tracking specific datasets, you can selectively add individual files.
